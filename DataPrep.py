{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load all modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "import random as rd\n",
    "\n",
    "# Print options\n",
    "np.set_printoptions(precision=4, threshold=10000, linewidth=160, edgeitems=999, suppress=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 160)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.set_option('precision', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#values of titanic set now need to be transformed. \n",
    "\n",
    "#Generate features from Cabin variable. Sparsely populated variable composed of letter&number:\n",
    "def getCabinLetter(cabin):\n",
    "    # finds letter component of Cabin variable (y axis of ship)\n",
    "    match = re.compile(\"([a-zA-z]+)\").search(cabin)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return 'U'\n",
    "\n",
    "def getCabinNumber(cabin):\n",
    "    # finds the number component of Cabin variable (x axis of ship)\n",
    "    match = re.compile(\"([0-9]+)\").search(cabin)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "def processCabin():\n",
    "    global df\n",
    "    df['Cabin'][df.Cabin.isnull()] = 'U0'#replace null values of cabine with 'U0'\n",
    "    \n",
    "    #map alphabetical part of Cabin to unique number\n",
    "    df['CabinLetter'] = df['Cabin'].map(lambda x : re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    df['CabinLetter'] = pd.factorize(df['CabinLetter'])[0]\n",
    "    \n",
    "    #Create dummy variables for cabin letter: Cabin letter will prob denote position on ship (y axis)\n",
    "    if keep_binary:\n",
    "        cletters = pd.get_dummies(df['CabinLetter']).rename(columns=lambda x: 'CabinLetter_' + str(x))\n",
    "        df=pd.concat([df,cletters], axis=1)\n",
    "        \n",
    "    #Create feature for numerical portion of Cabin. number will denote xaxis on ship\n",
    "    df['CabinNumber'] = df['Cabin'].map(lambda x: getCabinNumber(x)).astype(int) + 1\n",
    "    if keep_scaled:\n",
    "        scaler =preprocessing.StandardScaler()\n",
    "        df['CabinNumber_scaled'] = scaler.fit_transform(df['CabinNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform the Ticket variable\n",
    "#Tickets are alphanumeric. Letters followed by numbers STON & SOTON prefixs are equivalent, '.' are also equivalents\n",
    "\n",
    "#find letter component of ticket variable\n",
    "def getTicketPrefix(ticket):\n",
    "    match = re.compile(\"([a-zA-Z\\.\\/]+)\").search(ticket)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return 'U'\n",
    "    \n",
    "#find digit component of ticket variable\n",
    "def getTicketNumber(ticket):\n",
    "    match = re.compile(\"([\\d]+$)\").search(ticket)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "def processTicket():\n",
    "    global df\n",
    "    df['TicketPrefix'] = df['Ticket'].map(lambda x: getTicketPrefix(x.upper()))\n",
    "    df['TicketPrefix'] = df['TicketPrefix'].map(lambda x: re.sub('[\\.?/\\?]', '',x))\n",
    "    df['TicketPrefix'] = df['TicketPrefix'].map(lambda x: re.sub('STON', 'SOTON', x))\n",
    "    \n",
    "    df['TicketPrefixID'] = pd.factorize(df['TicketPrefix'])[0]\n",
    "    \n",
    "    #dummy variables for ticket letter: Then drop the Prefix \n",
    "    if keep_binary:\n",
    "        prefixes = pd.get_dummies(df['TicketPrefix']).rename(columns=lambda x: 'TicketPrefix_' + str(x))\n",
    "        df = pd.concat([df, prefixes], axis =1)\n",
    "    \n",
    "    df.drop(['TicketPrefix'], axis=1, inplace = True)\n",
    "    \n",
    "    #get the ticket numbers, the number of digits and first digit of all tickets\n",
    "    df['TicketNumber'] = df['Ticket'].map(lambda x: getTicketNumber(x))\n",
    "    df['TicketNumberDigits'] = df['TicketNumber'].map(lambda x: len(x)).astype(np.int)\n",
    "    df['TicketNumberStart'] = df['TicketNumber'].map(lambda x: x[0:1]).astype(np.int)\n",
    "    \n",
    "    df['TicketNumber'] = df.TicketNumber.astype(np.int)\n",
    "    \n",
    "    if keep_scaled:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        df['TicketNumber_scaled'] = scaler.fit_transform(df['TicketNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform the Embarked variable into dummy variables\n",
    "def processEmbarked():    \n",
    "    global df\n",
    "    #replace missing values with common port & then map to number\n",
    "    df.Embarked[df.Embarked.isnull()] = df.Embarked.dropna().mode().values\n",
    "    df['Embarked'] = pd.factorize(df['Embarked'])[0]\n",
    "    \n",
    "    if keep_binary:\n",
    "        df=pd.concat([df, pd.get_dummies(df['Embarked']).rename(columns= lambda x: 'Embarked_' + str(x))], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform Passenger Class. Replace missing values with Mode & create dummy variable & scale.\n",
    "def processPClass():\n",
    "    global df\n",
    "    df.Pclass[df.Pclass.isnull()]=df.Pclass.dropna().mode().values\n",
    "    if keep_binary:\n",
    "        df = pd.concat([df, pd.get_dummies(df['Pclass']).rename(columns= lambda x: 'Pclass_' + str(x))], axis=1)\n",
    "    if keep_scaled:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        df['Pclass_scaled'] = scaler.fit_transform(df['Pclass'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tranform Sex variable into dummies\n",
    "def processSex():\n",
    "    global df\n",
    "    df['Gender'] = np.where(df['Sex'] == 'male',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform Age\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#populate missing ages via Random Forest Classifier\n",
    "def SetMissingAges():\n",
    "    #All features that will be included in Random Forest Regressor\n",
    "    age_df = df[['Age', 'Embarked', 'Fare', 'Parch', 'SibSp', 'Title_id', 'Pclass', 'Names', 'CabinLetter']]\n",
    "    #split between known & unknown Age\n",
    "    known_age = age_df.loc[(df.Age.notnull())]\n",
    "    unknown_age = age_df.loc[(df.Age.isnull())]\n",
    "    \n",
    "    y = known_age.values[:, 0] #takes age values of known and sets this as y component\n",
    "    x = known_age.values[:, 1::] #takes the rest of the features and sets this as x component\n",
    "    \n",
    "    #create and fit the model\n",
    "    model = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    #Use model to predict missing values\n",
    "    predictedAges = model.predict(unknown_age.values[:, 1::])\n",
    "    \n",
    "    #replace missing ages with predicted ages\n",
    "    df.loc[(df.Age.isnull()), 'Age' ] = predictedAges \n",
    "\n",
    "#process Age\n",
    "def processAge():\n",
    "    global df\n",
    "    SetMissingAges()\n",
    "    \n",
    "    #center mean and scale based on standard deviation\n",
    "    if keep_scaled:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        df['Age_scaled'] = scaler.fit_transform(df['Age'])\n",
    "    \n",
    "    #dummy variable if subject is child. set child as age 13\n",
    "    df['isChild'] = np.where(df.Age < 13, 1, 0)\n",
    "    \n",
    "    #bin into quartiles & create dummies\n",
    "    df['Age_bin'] = pd.qcut(df['Age'], 4)\n",
    "    if keep_binary:\n",
    "        df = pd.concat([df, pd.get_dummies(df['Age_bin']).rename(columns=lambda x: 'Age_' + str(x))], axis=1)\n",
    "    \n",
    "    if keep_bins: #add 1 to bin so we start at 1 instead of 0\n",
    "        df['Age_bin_id'] = pd.factorize(df['Age_bin'])[0]+1\n",
    "        \n",
    "    if keep_bins and keep_scaled:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        df['Age_bin_id_scaled'] = scaler.fit_transform(df['Age_bin_id'])\n",
    "    \n",
    "    if not keep_strings:\n",
    "        df.drop('Age_bin', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform ticket fare. Replace missing values with median fare. \n",
    "\n",
    "def processFare():\n",
    "    global df\n",
    "    df['Fare'][np.isnan(df['Fare'])] = df['Fare'].median()\n",
    "    \n",
    "    #split into quartiles\n",
    "    \n",
    "    df['Fare_bin'] = pd.qcut(df['Fare'],4)\n",
    "    if keep_binary:\n",
    "        df = pd.concat([df, pd.get_dummies(df['Fare_bin']).rename(columns= lambda x: 'Fare_' + str(x))], axis=1)\n",
    "    \n",
    "    if keep_bins:\n",
    "        df['Fare_bin_id'] = pd.factorize(df['Fare_bin'])[0]+1\n",
    "    \n",
    "    if keep_scaled:\n",
    "        scaler=preprocessing.StandardScaler()\n",
    "        df['Fare_scaled'] = scaler.fit_transform(df['Fare'])\n",
    "    \n",
    "    if keep_bins and keep_scaled:\n",
    "        scaler=preprocessing.StandardScaler()\n",
    "        df['Fare_bin_id_scaled'] = scaler.fit_transform(df['Fare_bin_id'])\n",
    "        \n",
    "    if not keep_strings:\n",
    "        df.drop('Fare_bin', axis =1, inplace = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform Sib & Parch variables. Scale and dummy variables\n",
    "def processFamily():\n",
    "    global df\n",
    "    if keep_scaled:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        df['SibSp_scaled'] = scaler.fit_transform(df['SibSp'])\n",
    "        df['Parch_scaled'] = scaler.fit_transform(df['Parch'])\n",
    "        \n",
    "    if keep_binary:\n",
    "        sibsps = pd.get_dummies(df['SibSp']).rename(columns = lambda x: 'SibSp_' + str(x))\n",
    "        parchs = pd.get_dummies(df['Parch']).rename(columns = lambda x: 'Parch_' + str(x))\n",
    "        df = pd.concat([df, sibsps, parchs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform the Name variable\n",
    "def processName():\n",
    "    global df\n",
    "    #map of how many different names passenger has\n",
    "    df['Names'] = df['Name'].map(lambda x: len(re.split(' ', x)))\n",
    "    \n",
    "    #map of each person's title\n",
    "    df['Title'] = df['Name'].map(lambda x: re.compile(\", (.*?)\\.\").findall(x)[0])\n",
    "    \n",
    "    df['Title'][df.Title == 'Jonkheer'] = 'Master'\n",
    "    df['Title'][df.Title.isin(['Ms', 'Mlle'])] = 'Miss'\n",
    "    df['Title'][df.Title == 'Mme'] = 'Mrs'\n",
    "    df['Title'][df.Title.isin(['Capt', 'Don', 'Major', 'Col', 'Sir'])] = 'Sir'\n",
    "    df['Title'][df.Title.isin(['Dona', 'Lady', 'the Countess'])] = 'Lady'\n",
    "    \n",
    "    #dummy variables\n",
    "    if keep_binary:\n",
    "        df = pd.concat([df, pd.get_dummies(df['Title']).rename(columns =lambda x: 'Title_'+ str(x))], axis =1)\n",
    "        \n",
    "    if keep_scaled:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        df['Names_scaled'] = scaler.fit_transform(df['Names'])\n",
    "        \n",
    "    if keep_bins:\n",
    "        df['Title_id'] = pd.factorize(df['Title'])[0]+1\n",
    "        \n",
    "    if keep_bins and keep_scaled:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        df['Title_id_scaled'] = scaler.fit_transform(df['Title_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processDrops():\n",
    "    global df\n",
    "    #features from rawdata\n",
    "    rawdroplist = ['Name', 'Names', 'Title', 'Sex', 'SibSp', 'Parch', 'Pclass', 'Embarked', 'Cabin', 'CabinLetter', \\\n",
    "                  'CabinNumber', 'Age', 'Fare', 'Ticket', 'TicketNumber']\n",
    "    stringsdroplist = ['Title', 'Name', 'Cabin', 'Ticket', 'Sex', 'TicketNumber']\n",
    "    \n",
    "    if not keep_raw:\n",
    "        df.drop(rawdroplist, axis=1, inplace=True)\n",
    "    elif not keep_strings:\n",
    "        df.drop(stringsdroplist, axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetDataSets(binary=False, bins=False, scaled=False, strings=False, raw=True, pca=False, balanced=False):\n",
    "    input_df = pd.read_csv('rawdata/train.csv', header=0)\n",
    "    submit_df = pd.read_csv('rawdata/test.csv', header=0)\n",
    "    \n",
    "    global keep_binary, keep_bins, keep_scaled, keep_raw, keep_strings, df\n",
    "    keep_binary = binary\n",
    "    keep_bins = bins\n",
    "    keep_scaled = scaled\n",
    "    keep_raw = raw\n",
    "    keep_strings = strings\n",
    "    #will combine both test and train data into one\n",
    "    df = pd.concat([input_df, submit_df]) \n",
    "    \n",
    "    #re-number combined data-set\n",
    "    #reindex remaining columns\n",
    "    df.reset_index()\n",
    "    df = df.reindex_axis(input_df.columns, axis=1)\n",
    "    \n",
    "    processCabin()\n",
    "    processTicket()\n",
    "    processName()\n",
    "    processFare()    \n",
    "    processEmbarked()    \n",
    "    processFamily()\n",
    "    processSex()\n",
    "    processPClass()\n",
    "    processAge()\n",
    "    processDrops()\n",
    "    \n",
    "    #moved survived to first position\n",
    "    columns_list = list(df.columns.values)\n",
    "    columns_list.remove('Survived')\n",
    "    new_col_list = list(['Survived'])\n",
    "    new_col_list.extend(columns_list)\n",
    "    df = df.reindex(columns=new_col_list)\n",
    "    \n",
    "    print \"Starting with\", df.columns.size, \"manually generated features...\\n\", df.columns.values\n",
    "    \n",
    "    #find correlation (use spearman b/c relationship is not neccessarily linear)\n",
    "    \n",
    "    df_corr = df.drop(['Survived', 'PassengerId'],axis=1).corr(method='spearman')\n",
    "    #mask is created to ignore correlation with itself\n",
    "    mask = np.ones(df_corr.columns.size) - np.eye(df_corr.columns.size)\n",
    "    df_corr = mask * df_corr\n",
    "    \n",
    "    drops = []\n",
    "    # loop through each variable\n",
    "    for col in df_corr.columns.values:\n",
    "        # if we've already determined to drop the current variable, continue\n",
    "        if np.in1d([col],drops):\n",
    "            continue\n",
    "        \n",
    "        # find all the variables that are highly correlated with the current variable \n",
    "        # and add them to the drop list \n",
    "        corr = df_corr[abs(df_corr[col]) > 0.98].index\n",
    "        #print col, \"highly correlated with:\", corr\n",
    "        drops = np.union1d(drops, corr)\n",
    "    \n",
    "    print \"\\nDropping\", drops.shape[0], \"highly correlated features...\\n\" #, drops\n",
    "    df.drop(drops, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    # Split the data sets apart again, perform PCA/clustering/class balancing\n",
    "    \n",
    "    input_df = df[:input_df.shape[0]] \n",
    "    submit_df  = df[input_df.shape[0]:]\n",
    "    \n",
    "    if pca:\n",
    "        print \"reducing and clustering now...\"\n",
    "        input_df, submit_df = reduceAndCluster(input_df, submit_df)\n",
    "    else:\n",
    "        # drop the empty 'Survived' column for the test set that was created during set concatentation\n",
    "        submit_df.drop('Survived', axis=1, inplace=1)\n",
    "    \n",
    "    print \"\\n\", input_df.columns.size, \"initial features generated...\\n\" #, input_df.columns.values\n",
    "    \n",
    "    if balanced:\n",
    "        # Undersample training examples of passengers who did not survive\n",
    "        print 'Perished data shape:', input_df[input_df.Survived==0].shape\n",
    "        print 'Survived data shape:', input_df[input_df.Survived==1].shape\n",
    "        perished_sample = rd.sample(input_df[input_df.Survived==0].index, input_df[input_df.Survived==1].shape[0])\n",
    "        input_df = pd.concat([input_df.ix[perished_sample], input_df[input_df.Survived==1]])\n",
    "        input_df.sort(inplace=True)\n",
    "        print 'New even class training shape:', input_df.shape\n",
    "    \n",
    "    return input_df, submit_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceAndCluster(input_df, submit_df, clusters=3):\n",
    "    #Takes the train and test data frames and performs dimensionality reduction with PCA and clustering\n",
    "    # join the full data together\n",
    "    \n",
    "    df = pd.concat([input_df, submit_df])\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop('index', axis=1, inplace=True)\n",
    "    df = df.reindex_axis(input_df.columns, axis=1)\n",
    "    \n",
    "    # Series of labels\n",
    "    survivedSeries = pd.Series(df['Survived'], name='Survived')\n",
    "    \n",
    "    print df.head()\n",
    "    \n",
    "    # Split into feature and label arrays\n",
    "    X = df.values[:, 1::]\n",
    "    y = df.values[:, 0]\n",
    "    \n",
    "    print X[0:5]\n",
    "    \n",
    "    \n",
    "    # Minimum percentage of variance we want to be described by the resulting transformed components\n",
    "    variance_pct = .99\n",
    "    \n",
    "    # Create PCA object\n",
    "    pca = PCA(n_components=variance_pct)\n",
    "    \n",
    "    # Transform the initial features\n",
    "    X_transformed = pca.fit_transform(X,y)\n",
    "    \n",
    "    # Create a data frame from the PCA'd data\n",
    "    pcaDataFrame = pd.DataFrame(X_transformed)\n",
    "    \n",
    "    print pcaDataFrame.shape[1], \" components describe \", str(variance_pct)[1:], \"% of the variance\"\n",
    "    \n",
    "    # use basic clustering to group similar examples and save the cluster ID for each example in train and test\n",
    "    kmeans = KMeans(n_clusters=clusters, random_state=np.random.RandomState(4), init='random')\n",
    "    # # Perform clustering on labeled AND unlabeled data\n",
    "    # clusterIds = kmeans.fit_predict(X_pca)\n",
    "    #==============================================================================================================\n",
    "    \n",
    "    # Perform clustering on labeled data and then predict clusters for unlabeled data\n",
    "    trainClusterIds = kmeans.fit_predict(X_transformed[:input_df.shape[0]])\n",
    "    print \"clusterIds shape for training data: \", trainClusterIds.shape\n",
    "    #print \"trainClusterIds: \", trainClusterIds\n",
    "     \n",
    "    testClusterIds = kmeans.predict(X_transformed[input_df.shape[0]:])\n",
    "    print \"clusterIds shape for test data: \", testClusterIds.shape\n",
    "    #print \"testClusterIds: \", testClusterIds\n",
    "     \n",
    "    clusterIds = np.concatenate([trainClusterIds, testClusterIds])\n",
    "    print \"all clusterIds shape: \", clusterIds.shape\n",
    "    #print \"clusterIds: \", clusterIds\n",
    "    \n",
    "    \n",
    "    # construct the new DataFrame comprised of \"Survived\", \"ClusterID\", and the PCA features\n",
    "    clusterIdSeries = pd.Series(clusterIds, name='ClusterId')\n",
    "    df = pd.concat([survivedSeries, clusterIdSeries, pcaDataFrame], axis=1)\n",
    "    \n",
    "    # split into separate input and test sets again\n",
    "    input_df = df[:input_df.shape[0]]\n",
    "    submit_df = df[input_df.shape[0]:]\n",
    "    submit_df.reset_index(inplace=True)\n",
    "    submit_df.drop('index', axis=1, inplace=True)\n",
    "    submit_df.drop('Survived', axis=1, inplace=1)\n",
    "    \n",
    "    return input_df, submit_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:498: UserWarning: StandardScaler assumes floating point values as input, got int64\n",
      "  \"got %s\" % (estimator, X.dtype))\n",
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 105 manually generated features...\n",
      "['Survived' 'PassengerId' 'Pclass' 'Age' 'SibSp' 'Parch' 'Fare' 'Embarked' 'CabinLetter' 'CabinLetter_0' 'CabinLetter_1' 'CabinLetter_2' 'CabinLetter_3'\n",
      " 'CabinLetter_4' 'CabinLetter_5' 'CabinLetter_6' 'CabinLetter_7' 'CabinLetter_8' 'CabinNumber' 'CabinNumber_scaled' 'TicketPrefixID' 'TicketPrefix_A'\n",
      " 'TicketPrefix_AQ' 'TicketPrefix_AS' 'TicketPrefix_C' 'TicketPrefix_CA' 'TicketPrefix_CASOTON' 'TicketPrefix_FA' 'TicketPrefix_FC' 'TicketPrefix_FCC'\n",
      " 'TicketPrefix_LINE' 'TicketPrefix_LP' 'TicketPrefix_PC' 'TicketPrefix_PP' 'TicketPrefix_PPP' 'TicketPrefix_SC' 'TicketPrefix_SCA' 'TicketPrefix_SCAH'\n",
      " 'TicketPrefix_SCOW' 'TicketPrefix_SCPARIS' 'TicketPrefix_SOC' 'TicketPrefix_SOP' 'TicketPrefix_SOPP' 'TicketPrefix_SOTONO' 'TicketPrefix_SOTONOQ'\n",
      " 'TicketPrefix_SP' 'TicketPrefix_SWPP' 'TicketPrefix_U' 'TicketPrefix_WC' 'TicketPrefix_WEP' 'TicketNumberDigits' 'TicketNumberStart' 'TicketNumber_scaled'\n",
      " 'Names' 'Title_Dr' 'Title_Lady' 'Title_Master' 'Title_Miss' 'Title_Mr' 'Title_Mrs' 'Title_Rev' 'Title_Sir' 'Names_scaled' 'Title_id' 'Title_id_scaled'\n",
      " 'Fare_[0, 7.896]' 'Fare_(7.896, 14.454]' 'Fare_(14.454, 31.275]' 'Fare_(31.275, 512.329]' 'Fare_bin_id' 'Fare_scaled' 'Fare_bin_id_scaled' 'Embarked_0'\n",
      " 'Embarked_1' 'Embarked_2' 'SibSp_scaled' 'Parch_scaled' 'SibSp_0' 'SibSp_1' 'SibSp_2' 'SibSp_3' 'SibSp_4' 'SibSp_5' 'SibSp_8' 'Parch_0' 'Parch_1' 'Parch_2'\n",
      " 'Parch_3' 'Parch_4' 'Parch_5' 'Parch_6' 'Parch_9' 'Gender' 'Pclass_1' 'Pclass_2' 'Pclass_3' 'Pclass_scaled' 'Age_scaled' 'isChild' 'Age_[0.17, 21]'\n",
      " 'Age_(21, 28.232]' 'Age_(28.232, 38]' 'Age_(38, 80]' 'Age_bin_id' 'Age_bin_id_scaled']\n",
      "\n",
      "Dropping 14 highly correlated features...\n",
      "\n",
      "\n",
      "91 initial features generated...\n",
      "\n",
      "   Survived  Pclass  Age  SibSp  Parch     Fare  Embarked  CabinLetter  CabinLetter_1  CabinLetter_2  CabinLetter_3  CabinLetter_4  CabinLetter_5  CabinLetter_6  CabinLetter_7  CabinLetter_8  CabinNumber  TicketPrefixID  TicketPrefix_A  TicketPrefix_AQ  TicketPrefix_AS  TicketPrefix_C  TicketPrefix_CA  TicketPrefix_CASOTON  TicketPrefix_FA  TicketPrefix_FC  TicketPrefix_FCC  TicketPrefix_LINE  TicketPrefix_LP  TicketPrefix_PC  TicketPrefix_PP  TicketPrefix_PPP  TicketPrefix_SC  TicketPrefix_SCA  TicketPrefix_SCAH  TicketPrefix_SCOW  TicketPrefix_SCPARIS  TicketPrefix_SOC  TicketPrefix_SOP  TicketPrefix_SOPP  TicketPrefix_SOTONO  TicketPrefix_SOTONOQ  TicketPrefix_SP  TicketPrefix_SWPP  TicketPrefix_U  TicketPrefix_WC  TicketPrefix_WEP  TicketNumberDigits  TicketNumberStart  TicketNumber_scaled  Names  Title_Dr  Title_Lady  Title_Master  Title_Miss  Title_Mr  Title_Mrs  Title_Rev  Title_Sir  Title_id  Fare_[0, 7.896]  Fare_(7.896, 14.454]  Fare_(14.454, 31.275]  Fare_(31.275, 512.329]  Fare_bin_id  Embarked_1  Embarked_2  SibSp_1  SibSp_2  SibSp_3  SibSp_4  SibSp_5  SibSp_8  Parch_1  Parch_2  Parch_3  Parch_4  Parch_5  Parch_6  Parch_9  Gender  Pclass_1  Pclass_2  Pclass_3  isChild  Age_[0.17, 21]  Age_(21, 28.232]  Age_(28.232, 38]  Age_(38, 80]  Age_bin_id\n",
      "0         0       3   22      1      0   7.2500         0            0              0              0              0              0              0              0              0              0            1               0               1                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               0                0                 0                   5                  2              -0.4123      4         0           0             0           0         1          0          0          0         1                1                     0                      0                       0            1           0           0        1        0        0        0        0        0        0        0        0        0        0        0        0       1         0         0         1        0               0                 1                 0             0           1\n",
      "1         1       1   38      1      0  71.2833         1            1              1              0              0              0              0              0              0              0           86               1               0                0                0               0                0                     0                0                0                 0                  0                0                1                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               0                0                 0                   5                  1              -0.4180      7         0           0             0           0         0          1          0          0         2                0                     0                      0                       1            2           1           0        1        0        0        0        0        0        0        0        0        0        0        0        0       0         1         0         0        0               0                 0                 1             0           2\n",
      "2         1       3   26      0      0   7.9250         0            0              0              0              0              0              0              0              0              0            1               2               0                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    1                     0                0                  0               0                0                 0                   7                  3               4.4371      3         0           0             0           1         0          0          0          0         3                0                     1                      0                       0            3           0           0        0        0        0        0        0        0        0        0        0        0        0        0        0       0         0         0         1        0               0                 1                 0             0           1\n",
      "3         1       1   35      1      0  53.1000         0            1              1              0              0              0              0              0              0              0          124               3               0                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               1                0                 0                   6                  1              -0.2665      7         0           0             0           0         0          1          0          0         2                0                     0                      0                       1            2           0           0        1        0        0        0        0        0        0        0        0        0        0        0        0       0         1         0         0        0               0                 0                 1             0           2\n",
      "4         0       3   35      0      0   8.0500         0            0              0              0              0              0              0              0              0              0            1               3               0                0                0               0                0                     0                0                0                 0                  0                0                0                0                 0                0                 0                  0                  0                     0                 0                 0                  0                    0                     0                0                  0               1                0                 0                   6                  3               0.1423      4         0           0             0           0         1          0          0          0         1                0                     1                      0                       0            3           0           0        0        0        0        0        0        0        0        0        0        0        0        0        0       1         0         0         1        0               0                 0                 1             0           2\n",
      "[[   3.       22.        1.        0.        7.25      0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.\n",
      "     0.        1.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        5.        2.\n",
      "    -0.4123    4.        0.        0.        0.        0.        1.        0.        0.        0.        1.        1.        0.        0.        0.        1.\n",
      "     0.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.\n",
      "     0.        0.        1.        0.        0.        1.        0.        0.        1.    ]\n",
      " [   1.       38.        1.        0.       71.2833    1.        1.        1.        0.        0.        0.        0.        0.        0.        0.       86.\n",
      "     1.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        5.        1.\n",
      "    -0.418     7.        0.        0.        0.        0.        0.        1.        0.        0.        2.        0.        0.        0.        1.        2.\n",
      "     1.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        0.        0.        0.        0.        0.        1.        0.        2.    ]\n",
      " [   3.       26.        0.        0.        7.925     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.\n",
      "     2.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        1.        0.        0.        0.        0.        0.        0.        7.        3.\n",
      "     4.4371    3.        0.        0.        0.        1.        0.        0.        0.        0.        3.        0.        1.        0.        0.        3.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        1.        0.        0.        1.        0.        0.        1.    ]\n",
      " [   1.       35.        1.        0.       53.1       0.        1.        1.        0.        0.        0.        0.        0.        0.        0.      124.\n",
      "     3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.        0.        6.        1.\n",
      "    -0.2665    7.        0.        0.        0.        0.        0.        1.        0.        0.        2.        0.        0.        0.        1.        2.\n",
      "     0.        0.        1.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     1.        0.        0.        0.        0.        0.        1.        0.        2.    ]\n",
      " [   3.       35.        0.        0.        8.05      0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.\n",
      "     3.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.        0.        0.        6.        3.\n",
      "     0.1423    4.        0.        0.        0.        0.        1.        0.        0.        0.        1.        0.        1.        0.        0.        3.\n",
      "     0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        0.        1.\n",
      "     0.        0.        1.        0.        0.        0.        1.        0.        2.    ]]\n",
      "3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  components describe  .99 % of the variance\n",
      "clusterIds shape for training data:  (891,)\n",
      "clusterIds shape for test data:  (418,)\n",
      "all clusterIds shape:  (1309,)\n",
      "Labeled survived counts :\n",
      "0    0.6162\n",
      "1    0.3838\n",
      "Name: Survived, dtype: float64\n",
      "Labeled cluster counts :\n",
      "0    0.8249\n",
      "1    0.1526\n",
      "2    0.0224\n",
      "Name: ClusterId, dtype: float64\n",
      "Unlabeled cluster counts:\n",
      "0    0.8373\n",
      "1    0.1196\n",
      "2    0.0431\n",
      "Name: ClusterId, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dentonzhao/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    train, test = GetDataSets(bins=True, scaled=True, binary=True)\n",
    "    drop_list =['PassengerId']\n",
    "    train.drop(drop_list, axis=1, inplace=1)\n",
    "    test.drop(drop_list, axis =1, inplace=1)\n",
    "    \n",
    "    train, test = reduceAndCluster(train, test)\n",
    "    \n",
    "    print \"Labeled survived counts :\\n\", pd.value_counts(train['Survived'])/train.shape[0]\n",
    "    print \"Labeled cluster counts :\\n\", pd.value_counts(train['ClusterId'])/train.shape[0]\n",
    "    print \"Unlabeled cluster counts:\\n\", pd.value_counts(test['ClusterId'])/test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
